{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 12. Information Retrieval"
   ],
   "metadata": {
    "id": "uQrkIu-o7lHH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalacja bibliotek"
   ],
   "metadata": {
    "id": "1l_O3BCI7ngm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDnKAQre7XoS",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.7 langchain-community==0.3.5 pypdf==5.1.0 wikipedia==1.4.0 openai==1.54.3 tiktoken==0.8.0 chromadb==0.5.18 langchain-openai==0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - Model Przestrzeni Wektorowej\n",
    "\n",
    "Napisz funkcję, która przyjmuje listę dokumentów i zapytanie, a zwraca listę dokumentów posortowanych według podobieństwa cosinusowego z zapytaniem. "
   ],
   "metadata": {
    "id": "UW4Rf28_96QU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def tf_idf_retrieval(documents: list[str], query: str) -> pd.DataFrame:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "documents = [\n",
    "    \"AI in healthcare improves diagnostics and patient care. AI algorithms analyze medical medical datasets efficiently\",\n",
    "    \"AI enhances the e-commerce experience. AI systems recommend products, predict customer preferences, and increase sales. AI also personalizes online shopping\",\n",
    "    \"Healthcare and AI are transforming diagnostics. Healthcare diagnostics benefit greatly from AI’s ability to process large datasets efficiently\",\n",
    "    \"AI applications in healthcare include diagnostics, treatment planning, and patient monitoring. Healthcare systems increasingly rely on AI advancements\",\n",
    "    \"AI in transportation is reshaping the future with self-driving cars, traffic optimization, and route planning. AI is making travel safer and more efficient\"\n",
    "]\n",
    "\n",
    "query = \"AI in healthcare diagnostics\"\n",
    "results = tf_idf_retrieval(documents, query)\n",
    "print(results)"
   ],
   "metadata": {
    "id": "mp1zj8O073sU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 2 - Normalizacja\n",
    "\n",
    "Napisz funkcję, która normalizuje tekst (zmienia na małe litery, usuwa znaki specjalne i nadmiarowe spacje)."
   ],
   "metadata": {
    "id": "mfb0sSHVW6jd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "text = \"Information     Retrieval is... FUN!!!\"\n",
    "normalized_text = normalize(text)\n",
    "print(normalized_text)"
   ],
   "metadata": {
    "id": "6fwc7GAjX3Yy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 3 - Tokenizacja\n",
    "\n",
    "Napisz funkcję, która tokenizuje tekst za pomocą biblioteki nltk."
   ],
   "metadata": {
    "id": "cuE76_6PX6li"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def word_tokenization(doc: str) -> list[str]:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "text = \"Dr. Smith said, 'Information Retrieval is fascinating!'\"\n",
    "tokens = word_tokenization(text)\n",
    "print(tokens)"
   ],
   "metadata": {
    "id": "NJypEAcXYGY8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 4 - Usuwanie stop-words\n",
    "\n",
    "Napisz funkcję, która usuwa stop-wordy (słowa nieinformatywne) z listy tokenów.\n",
    "\n"
   ],
   "metadata": {
    "id": "643zmBTyY8FV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(tokens: list[str]) -> list[str]:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "tokens = [\"this\", \"is\", \"an\", \"example\", \"of\", \"stop-word\", \"removal\", \"for\", \"information\", \"retrieval\"]\n",
    "filtered_tokens = remove_stop_words(tokens)\n",
    "print(filtered_tokens)"
   ],
   "metadata": {
    "id": "h3LYQPr3ZITW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 5 - Embeddingi Zdań (Sentence Embeddings)\n",
    "\n",
    "Napisz funkcję, która oblicza embeddingi dla listy zdań za pomocą modelu SentenceTransformer. Zobacz jak wyglada wizualizacja tych embeddingów po zmniejszeniu liczby wymiarów."
   ],
   "metadata": {
    "id": "4jhBaT7-bdMc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_sentence_embeddings(sentences: list[str]) -> np.array:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "sentences = [\n",
    "    \"Information Retrieval is fascinating\",\n",
    "    \"Retrieval systems retrieve documents\",\n",
    "    \"I love playing football\",\n",
    "    \"Soccer is a great sport\",\n",
    "    \"Cooking is an enjoyable activity\",\n",
    "    \"Baking is part of cooking\",\n",
    "    \"A lovely day for a picnic\",\n",
    "    \"Cats are beautiful animals\"\n",
    "]\n",
    "\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "print(\"Embeddings Shape:\", sentence_embeddings.shape)"
   ],
   "metadata": {
    "id": "AieqGtJqbmRD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce Dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Plot the Embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x, y = reduced_embeddings[i]\n",
    "    plt.scatter(x, y, label=f\"Sentence {i+1}\")\n",
    "    plt.text(x + 0.01, y + 0.01, sentence, fontsize=9)\n",
    "\n",
    "plt.title(\"2D Visualization of Sentence Embeddings\")\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "KczI85tRcosl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 6 - Wektorowa Baza Danych\n",
    "\n",
    "Napisz dwie funkcje. Jedną która tworzy wektorową bazę danych w ChromaDB, dodaje dokumenty i drugą która przeszukuje ją na podstawie zapytania."
   ],
   "metadata": {
    "id": "qG89htgfZgOc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from chromadb import Client\n",
    "\n",
    "def create_index(index_name: str, documents: list[str]) -> Client:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "def query_index(index: Client, query: str, n_results: int) -> pd.DataFrame:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "documents = [\n",
    "    \"AI is the future of technology\",\n",
    "    \"The world is becoming more digital\",\n",
    "    \"Data science is the key to innovation\",\n",
    "    \"Remote work is the new normal\",\n",
    "    \"OpenAI provides powerful tools for developers\",\n",
    "    \"I like to code in Python\"\n",
    "]\n",
    "\n",
    "index_name = \"NAI\"\n",
    "index = create_index(index_name, documents)\n",
    "query = \"What is the future of technology?\"\n",
    "results = query_index(index, query, n_results=3)\n",
    "print(results)"
   ],
   "metadata": {
    "id": "Z8NnDQoIZtyG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 7 - Podział Tekstu (Chunking)\n",
    "\n",
    "Napisz funkcję, która dzieli tekst na chunki o stałej długości za pomocą CharacterTextSplitter."
   ],
   "metadata": {
    "id": "abMhW_ZWd1Ud"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> list[str]:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "text = \"This is a sample document for text chunking. The text will be split into smaller chunks of fixed length.\"\n",
    "text_chunks = chunk_text(text, chunk_size=30, chunk_overlap=10)\n",
    "print(\"Chunks:\")\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ],
   "metadata": {
    "id": "opWdl48FeAqM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie domowe - Image Embeddings\n",
    "\n",
    "Stwórz system IR, który będzie mógł przeszukiwać tekst książki. Skorzystaj z funkcji z poprzednich zadań.\n",
    "\n",
    "1.   *Pobierz zawartość strony internetowej.*\n",
    "2.   Przetwórz tekst (normalizacja, tokenizacja).\n",
    "3.   Podziel tekst na chunki o stałej długości.\n",
    "4.   Stwórz bazę danych wektorową w ChromaDB i dodaj chunki.\n",
    "5.   *Przeszukaj bazę danych wektorową na podstawie zapytania.*\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "aCtVrWFlekMM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Funkcja do pobierania i wyciągania tekstu ze strony\n",
    "def extract_text_from_url(url: str) -> str:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text\n",
    "\n",
    "# Funkcja do preprocessingu tekstu\n",
    "def preprocess_text(text: str) -> str:\n",
    "    pass\n",
    "\n",
    "# URL strony z przepisami\n",
    "url = \"https://www.allrecipes.com/recipe/23600/worlds-best-lasagna/\"\n",
    "text = extract_text_from_url(url)\n",
    "\n",
    "# Twoje rozwiązanie\n",
    "#__________________\n",
    "\n",
    "# Przeszukiwanie indeksu\n",
    "query = \"How long to cook lasagna?\"\n",
    "results = query_index(index, query, n_results=10)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "for doc in results[\"Document\"][0]:\n",
    "    print(doc, end=\"\")\n",
    "    # print(\"-\"*50)\n"
   ],
   "metadata": {
    "id": "C7dfHAnpg_BV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie dodatkowe\n",
    "\n",
    "Napisz dwie funkcje: jedna oblicza embeddingi obrazów za pomocą modelu CLIP, druga znajduje obraz najbardziej podobny do zapytania."
   ],
   "metadata": {
    "id": "cCNDmFS1oeVA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Przykładowe obrazy\n",
    "images = [\n",
    "    \"https://images.unsplash.com/2/03.jpg\",\n",
    "    \"https://images.unsplash.com/2/01.jpg\",\n",
    "    \"https://images.unsplash.com/2/06.jpg\",\n",
    "    \"https://images.unsplash.com/2/09.jpg\"\n",
    "]\n",
    "\n",
    "# query = \"Forest\"\n",
    "# query = \"Beach\"\n",
    "query = \"Path\"\n",
    "\n",
    "# Funkcja do pobierania obrazów\n",
    "def download_image(url: str) -> Image:\n",
    "    return Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Wyświetlanie obrazów\n",
    "fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "for i, image_url in enumerate(images):\n",
    "    image = download_image(image_url)\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(f\"Image {i+1}\")\n",
    "plt.show()\n",
    "\n",
    "# Funkcja do obliczania embeddingów obrazów\n",
    "def get_clip_image_embeddings(images: list[str]) -> np.array:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "# Funkcja do obliczania embeddingu zapytania\n",
    "def get_clip_text_embeddings(text: str) -> np.array:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "# Funkcja do znalezienia najbardziej podobnego obrazu\n",
    "def find_most_similar_image(image_embeddings: np.array, query_embedding: np.array) -> int:\n",
    "    # Twoje rozwiązanie\n",
    "    pass\n",
    "\n",
    "# Obliczanie embeddingów obrazów i zapytania\n",
    "image_embeddings = get_clip_image_embeddings(images)\n",
    "query_embedding = get_clip_text_embeddings(query)\n",
    "\n",
    "# Znalezienie najbardziej podobnego obrazu\n",
    "most_similar_idx = find_most_similar_image(image_embeddings, query_embedding)\n",
    "print(f\"Najbardziej podobny obraz: Image {most_similar_idx + 1}\")"
   ],
   "metadata": {
    "id": "iIxTRwCsjH3I"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
